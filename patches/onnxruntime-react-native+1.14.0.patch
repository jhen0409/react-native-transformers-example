diff --git a/node_modules/onnxruntime-react-native/android/build.gradle b/node_modules/onnxruntime-react-native/android/build.gradle
index 4c8a318..65b58c1 100644
--- a/node_modules/onnxruntime-react-native/android/build.gradle
+++ b/node_modules/onnxruntime-react-native/android/build.gradle
@@ -135,5 +135,8 @@ dependencies {
 
   // Mobile build:
   // implementation "com.microsoft.onnxruntime:onnxruntime-mobile:latest.integration@aar"
-  implementation "com.microsoft.onnxruntime:onnxruntime-android:latest.integration@aar"
+  // implementation "com.microsoft.onnxruntime:onnxruntime-android:latest.integration@aar"
+  // Use local AAR file
+	implementation project(":onnxruntime-patched")
+
 }
diff --git a/node_modules/onnxruntime-react-native/android/src/main/java/ai/onnxruntime/reactnative/TensorHelper.java b/node_modules/onnxruntime-react-native/android/src/main/java/ai/onnxruntime/reactnative/TensorHelper.java
index 500141a..49b3abd 100644
--- a/node_modules/onnxruntime-react-native/android/src/main/java/ai/onnxruntime/reactnative/TensorHelper.java
+++ b/node_modules/onnxruntime-react-native/android/src/main/java/ai/onnxruntime/reactnative/TensorHelper.java
@@ -164,7 +164,11 @@ public class TensorHelper {
       tensor = OnnxTensor.createTensor(ortEnvironment, buffer, dims, OnnxJavaType.UINT8);
       break;
     }
-    case ONNX_TENSOR_ELEMENT_DATA_TYPE_BOOL:
+    case ONNX_TENSOR_ELEMENT_DATA_TYPE_BOOL: {
+      ByteBuffer buffer = values;
+      tensor = OnnxTensor.createTensor(ortEnvironment, buffer, dims, OnnxJavaType.BOOL);
+      break;
+    }
     case ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16:
     case ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16:
     case ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT32:
diff --git a/node_modules/onnxruntime-react-native/cpp/ThreadPool.cpp b/node_modules/onnxruntime-react-native/cpp/ThreadPool.cpp
new file mode 100644
index 0000000..848d5f6
--- /dev/null
+++ b/node_modules/onnxruntime-react-native/cpp/ThreadPool.cpp
@@ -0,0 +1,92 @@
+#include "ThreadPool.h"
+
+ThreadPool::ThreadPool() : done(false)
+{
+  // This returns the number of threads supported by the system. If the
+  // function can't figure out this information, it returns 0. 0 is not good,
+  // so we create at least 1
+  auto numberOfThreads = std::thread::hardware_concurrency();
+  if (numberOfThreads == 0)
+  {
+    numberOfThreads = 1;
+  }
+
+  for (unsigned i = 0; i < numberOfThreads; ++i)
+  {
+    // The threads will execute the private member `doWork`. Note that we need
+    // to pass a reference to the function (namespaced with the class name) as
+    // the first argument, and the current object as second argument
+    threads.push_back(std::thread(&ThreadPool::doWork, this));
+  }
+}
+
+// The destructor joins all the threads so the program can exit gracefully.
+// This will be executed if there is any exception (e.g. creating the threads)
+ThreadPool::~ThreadPool()
+{
+  // So threads know it's time to shut down
+  done = true;
+
+  // Wake up all the threads, so they can finish and be joined
+  workQueueConditionVariable.notify_all();
+  for (auto &thread : threads)
+  {
+    if (thread.joinable())
+    {
+      thread.join();
+    }
+  }
+}
+
+// This function will be called by the server every time there is a request
+// that needs to be processed by the thread pool
+void ThreadPool::queueWork(std::function<void(void)> task)
+{
+  // Grab the mutex
+  std::lock_guard<std::mutex> g(workQueueMutex);
+
+  // Push the request to the queue
+  workQueue.push(task);
+
+  // Notify one thread that there are requests to process
+  workQueueConditionVariable.notify_one();
+}
+
+// Function used by the threads to grab work from the queue
+void ThreadPool::doWork()
+{
+  // Loop while the queue is not destructing
+  while (!done)
+  {
+    std::function<void(void)> task;
+
+    // Create a scope, so we don't lock the queue for longer than necessary
+    {
+      std::unique_lock<std::mutex> g(workQueueMutex);
+      workQueueConditionVariable.wait(g, [&]
+                                      {
+        // Only wake up if there are elements in the queue or the program is
+        // shutting down
+        return !workQueue.empty() || done; });
+
+      // If we are shutting down exit witout trying to process more work
+      if (done)
+      {
+        break;
+      }
+
+      task = workQueue.front();
+      workQueue.pop();
+    }
+    ++busy;
+    task();
+    --busy;
+  }
+}
+
+void ThreadPool::waitFinished()
+{
+  std::unique_lock<std::mutex> g(workQueueMutex);
+  workQueueConditionVariable.wait(g, [&]
+                                  { return workQueue.empty() && (busy == 0); });
+}
\ No newline at end of file
diff --git a/node_modules/onnxruntime-react-native/cpp/ThreadPool.h b/node_modules/onnxruntime-react-native/cpp/ThreadPool.h
new file mode 100644
index 0000000..4369894
--- /dev/null
+++ b/node_modules/onnxruntime-react-native/cpp/ThreadPool.h
@@ -0,0 +1,50 @@
+//
+//  ThreadPool.hpp
+//  react-native-quick-sqlite
+//
+//  Created by Oscar on 13.03.22.
+//
+
+#ifndef ThreadPool_hpp
+#define ThreadPool_hpp
+
+#include <condition_variable>
+#include <exception>
+#include <mutex>
+#include <queue>
+#include <stdio.h>
+#include <thread>
+#include <vector>
+
+class ThreadPool
+{
+public:
+  ThreadPool();
+  ~ThreadPool();
+  void queueWork(std::function<void(void)> task);
+  void waitFinished();
+
+private:
+  unsigned int busy;
+  // This condition variable is used for the threads to wait until there is work
+  // to do
+  std::condition_variable_any workQueueConditionVariable;
+
+  // We store the threads in a vector, so we can later stop them gracefully
+  std::vector<std::thread> threads;
+
+  // Mutex to protect workQueue
+  std::mutex workQueueMutex;
+
+  // Queue of requests waiting to be processed
+  std::queue<std::function<void(void)>> workQueue;
+
+  // This will be set to true when the thread pool is shutting down. This tells
+  // the threads to stop looping and finish
+  bool done;
+
+  // Function used by the threads to grab work from the queue
+  void doWork();
+};
+
+#endif /* ThreadPool_hpp */
\ No newline at end of file
diff --git a/node_modules/onnxruntime-react-native/ios/OnnxruntimeModule.mm b/node_modules/onnxruntime-react-native/ios/OnnxruntimeModule.mm
index fe7bf48..b57073b 100644
--- a/node_modules/onnxruntime-react-native/ios/OnnxruntimeModule.mm
+++ b/node_modules/onnxruntime-react-native/ios/OnnxruntimeModule.mm
@@ -6,7 +6,23 @@
 
 #import <Foundation/Foundation.h>
 #import <React/RCTLog.h>
-#import <onnxruntime/onnxruntime_cxx_api.h>
+#import <React/RCTBridge+Private.h>
+#import <React/RCTBlobManager.h>
+#import <jsi/jsi.h>
+
+// Note: Using below syntax for including ort c api and ort extensions headers to resolve a compiling error happened
+// in an expo react native ios app when ort extensions enabled (a redefinition error of multiple object types defined
+// within ORT C API header). It's an edge case that compiler allows both ort c api headers to be included when #include
+// syntax doesn't match. For the case when extensions not enabled, it still requires a onnxruntime prefix directory for
+// searching paths. Also in general, it's a convention to use #include for C/C++ headers rather then #import. See:
+// https://google.github.io/styleguide/objcguide.html#import-and-include
+// https://microsoft.github.io/objc-guide/Headers/ImportAndInclude.html
+#ifdef ORT_ENABLE_EXTENSIONS
+#include "onnxruntime_cxx_api.h"
+#include "onnxruntime_extensions.h"
+#else
+#include "onnxruntime/onnxruntime_cxx_api.h"
+#endif
 
 @implementation OnnxruntimeModule
 
@@ -22,6 +38,13 @@ @implementation OnnxruntimeModule
 static NSMutableDictionary *sessionMap = [NSMutableDictionary dictionary];
 static Ort::AllocatorWithDefaultOptions ortAllocator;
 
+static int nextSessionId = 0;
+- (NSString *)getNextSessionKey {
+  NSString *key = @(nextSessionId).stringValue;
+  nextSessionId++;
+  return key;
+}
+
 RCT_EXPORT_MODULE(Onnxruntime)
 
 /**
@@ -43,7 +66,35 @@ @implementation OnnxruntimeModule
     NSDictionary *resultMap = [self loadModel:modelPath options:options];
     resolve(resultMap);
   } @catch (...) {
-    reject(@"onnxruntime", @"can't load model", nil);
+    reject(@"onnxruntime", @"failed to load model", nil);
+  }
+}
+
+/**
+ * React native binding API to load a model using BASE64 encoded model data string.
+ *
+ * @param modelData the BASE64 encoded model data string
+ * @param options onnxruntime session options
+ * @param resolve callback for returning output back to react native js
+ * @param reject callback for returning an error back to react native js
+ * @note when run() is called, the same modelPath must be passed into the first parameter.
+ */
+RCT_EXPORT_METHOD(loadModelFromBlob
+                  : (NSDictionary *)modelDataBlob options
+                  : (NSDictionary *)options resolver
+                  : (RCTPromiseResolveBlock)resolve rejecter
+                  : (RCTPromiseRejectBlock)reject) {
+  @try {
+    RCTBlobManager* blobManager = [[RCTBridge currentBridge] moduleForClass:RCTBlobManager.class];
+    NSString *blobId = [modelDataBlob objectForKey:@"blobId"];
+    long size = [[modelDataBlob objectForKey:@"size"] longValue];
+    long offset = [[modelDataBlob objectForKey:@"offset"] longValue];
+    auto modelData = [blobManager resolve:blobId offset:offset size:size];
+    NSDictionary *resultMap = [self loadModelFromBuffer:modelData options:options];
+    resolve(resultMap);
+    [blobManager remove:blobId];
+  } @catch (...) {
+    reject(@"onnxruntime", @"failed to load model from buffer", nil);
   }
 }
 
@@ -68,49 +119,75 @@ @implementation OnnxruntimeModule
     NSDictionary *resultMap = [self run:url input:input output:output options:options];
     resolve(resultMap);
   } @catch (...) {
-    reject(@"onnxruntime", @"can't run model", nil);
+    reject(@"onnxruntime", @"failed to run model", nil);
   }
 }
 
 /**
  * Load a model using given model path.
  *
- * @param modelPath a model file location. it's used as a key when multiple sessions are created, i.e. multiple models
- * are loaded.
- * @param options onnxruntime session options
+ * @param modelPath a model file location.
+ * @param options onnxruntime session options.
  * @note when run() is called, the same modelPath must be passed into the first parameter.
  */
 - (NSDictionary *)loadModel:(NSString *)modelPath options:(NSDictionary *)options {
-  NSValue *value = [sessionMap objectForKey:modelPath];
-  SessionInfo *sessionInfo = nullptr;
-  if (value == nil) {
-    sessionInfo = new SessionInfo();
+  return [self loadModelImpl:modelPath modelData:nil options:options];
+}
 
-    Ort::SessionOptions sessionOptions = [self parseSessionOptions:options];
-    sessionInfo->session.reset(new Ort::Session(*ortEnv, [modelPath UTF8String], sessionOptions));
+/**
+ * Load a model using given model data array
+ *
+ * @param modelData the model data buffer.
+ * @param options onnxruntime session options
+ */
+- (NSDictionary *)loadModelFromBuffer:(NSData *)modelData options:(NSDictionary *)options {
+  return [self loadModelImpl:@"" modelData:modelData options:options];
+}
 
-    sessionInfo->inputNames.reserve(sessionInfo->session->GetInputCount());
-    for (size_t i = 0; i < sessionInfo->session->GetInputCount(); ++i) {
-      auto inputName = sessionInfo->session->GetInputNameAllocated(i, ortAllocator);
-      sessionInfo->inputNames.emplace_back(inputName.get());
-      sessionInfo->inputNames_ptrs.emplace_back(std::move(inputName));
-    }
+/**
+ * Load model implementation method given either model data array or model path
+ *
+ * @param modelPath the model file location.
+ * @param modelData the model data buffer.
+ * @param options onnxruntime session options.
+ */
+- (NSDictionary *)loadModelImpl:(NSString *)modelPath modelData:(NSData *)modelData options:(NSDictionary *)options {
+  SessionInfo *sessionInfo = nullptr;
+  sessionInfo = new SessionInfo();
+  Ort::SessionOptions sessionOptions = [self parseSessionOptions:options];
 
-    sessionInfo->outputNames.reserve(sessionInfo->session->GetOutputCount());
-    for (size_t i = 0; i < sessionInfo->session->GetOutputCount(); ++i) {
-      auto outputName = sessionInfo->session->GetOutputNameAllocated(i, ortAllocator);
-      sessionInfo->outputNames.emplace_back(outputName.get());
-      sessionInfo->outputNames_ptrs.emplace_back(std::move(outputName));
-    }
+#ifdef ORT_ENABLE_EXTENSIONS
+  Ort::ThrowOnError(RegisterCustomOps(sessionOptions, OrtGetApiBase()));
+#endif
 
-    value = [NSValue valueWithPointer:(void *)sessionInfo];
-    sessionMap[modelPath] = value;
+  if (modelData == nil) {
+    sessionInfo->session.reset(new Ort::Session(*ortEnv, [modelPath UTF8String], sessionOptions));
   } else {
-    sessionInfo = (SessionInfo *)[value pointerValue];
+    NSUInteger dataLength = [modelData length];
+    Byte *modelBytes = (Byte *)[modelData bytes];
+    sessionInfo->session.reset(new Ort::Session(*ortEnv, modelBytes, (size_t)dataLength, sessionOptions));
   }
 
+  sessionInfo->inputNames.reserve(sessionInfo->session->GetInputCount());
+  for (size_t i = 0; i < sessionInfo->session->GetInputCount(); ++i) {
+    auto inputName = sessionInfo->session->GetInputNameAllocated(i, ortAllocator);
+    sessionInfo->inputNames.emplace_back(inputName.get());
+    sessionInfo->inputNames_ptrs.emplace_back(std::move(inputName));
+  }
+
+  sessionInfo->outputNames.reserve(sessionInfo->session->GetOutputCount());
+  for (size_t i = 0; i < sessionInfo->session->GetOutputCount(); ++i) {
+    auto outputName = sessionInfo->session->GetOutputNameAllocated(i, ortAllocator);
+    sessionInfo->outputNames.emplace_back(outputName.get());
+    sessionInfo->outputNames_ptrs.emplace_back(std::move(outputName));
+  }
+
+  NSString *key = [self getNextSessionKey];
+  NSValue *value = [NSValue valueWithPointer:(void *)sessionInfo];
+  sessionMap[key] = value;
+
   NSMutableDictionary *resultMap = [NSMutableDictionary dictionary];
-  resultMap[@"key"] = modelPath;
+  resultMap[@"key"] = key;
 
   NSMutableArray *inputNames = [NSMutableArray array];
   for (auto inputName : sessionInfo->inputNames) {
@@ -276,4 +353,74 @@ - (void)dealloc {
   }
 }
 
+
+RCT_EXPORT_BLOCKING_SYNCHRONOUS_METHOD(install)
+{
+  RCTBridge* bridge = [RCTBridge currentBridge];
+  RCTCxxBridge* cxxBridge = (RCTCxxBridge*)bridge;
+  if (cxxBridge == nil) {
+    return @false;
+  }
+
+  using namespace facebook;
+
+  auto jsiRuntime = (jsi::Runtime*) cxxBridge.runtime;
+  if (jsiRuntime == nil) {
+    return @false;
+  }
+
+  auto& runtime = *jsiRuntime;
+
+  auto getTensorArrayBuffer = jsi::Function::createFromHostFunction(runtime,
+                                                                     jsi::PropNameID::forUtf8(runtime, "getTensorArrayBuffer"),
+                                                                     1,
+                                                                     [](jsi::Runtime& runtime,
+                                                                        const jsi::Value& thisArg,
+                                                                        const jsi::Value* args,
+                                                                        size_t count) -> jsi::Value {
+    auto data = args[0].asObject(runtime);
+    auto blobId = data.getProperty(runtime, "blobId").asString(runtime).utf8(runtime);
+    auto size = data.getProperty(runtime, "size").asNumber();
+    auto offset = data.getProperty(runtime, "offset").asNumber();
+
+    RCTBlobManager* blobManager = [[RCTBridge currentBridge] moduleForClass:RCTBlobManager.class];
+    NSString *blobIdStr = [NSString stringWithUTF8String:blobId.c_str()];
+    auto blob = [blobManager resolve:blobIdStr offset:(long)offset size:(long)size];
+
+    jsi::Function arrayBufferCtor = runtime.global().getPropertyAsFunction(runtime, "ArrayBuffer");
+    jsi::Object o = arrayBufferCtor.callAsConstructor(runtime, (int)blob.length).getObject(runtime);
+    jsi::ArrayBuffer buf = o.getArrayBuffer(runtime);
+    memcpy(buf.data(runtime), blob.bytes, blob.length);
+    [blobManager remove:blobIdStr];
+    return buf;
+  });
+  runtime.global().setProperty(runtime, "__ONNXRUNTIME_getTensorArrayBuffer", getTensorArrayBuffer);
+
+  auto setTensorArrayBuffer = jsi::Function::createFromHostFunction(runtime,
+                                                                     jsi::PropNameID::forUtf8(runtime, "setTensorArrayBuffer"),
+                                                                     1,
+                                                                     [](jsi::Runtime& runtime,
+                                                                        const jsi::Value& thisArg,
+                                                                        const jsi::Value* args,
+                                                                        size_t count) -> jsi::Value {
+    auto arrayBuffer = args[0].asObject(runtime).getArrayBuffer(runtime);
+    auto size = arrayBuffer.length(runtime);
+    NSData* data = [NSData dataWithBytes:arrayBuffer.data(runtime) length:size];
+
+    RCTBlobManager* blobManager = [[RCTBridge currentBridge] moduleForClass:RCTBlobManager.class];
+    NSString* blobId = [blobManager store:data];
+
+    jsi::Object result(runtime);
+    auto blobIdString = jsi::String::createFromUtf8(runtime, [blobId cStringUsingEncoding:NSUTF8StringEncoding]);
+    result.setProperty(runtime, "blobId", blobIdString);
+    result.setProperty(runtime, "offset", jsi::Value(0));
+    result.setProperty(runtime, "size", jsi::Value(static_cast<double>(size)));
+    return result;
+  });
+
+  runtime.global().setProperty(runtime, "__ONNXRUNTIME_setTensorArrayBuffer", setTensorArrayBuffer);
+
+  return @true;
+}
+
 @end
diff --git a/node_modules/onnxruntime-react-native/ios/TensorHelper.h b/node_modules/onnxruntime-react-native/ios/TensorHelper.h
index f0936cc..3fc0bf0 100644
--- a/node_modules/onnxruntime-react-native/ios/TensorHelper.h
+++ b/node_modules/onnxruntime-react-native/ios/TensorHelper.h
@@ -6,6 +6,7 @@
 
 #import <Foundation/Foundation.h>
 #import <onnxruntime/onnxruntime_cxx_api.h>
+#import <jsi/jsi.h>
 
 @interface TensorHelper : NSObject
 
@@ -22,6 +23,11 @@ FOUNDATION_EXPORT NSString* const JsTensorTypeFloat;
 FOUNDATION_EXPORT NSString* const JsTensorTypeDouble;
 FOUNDATION_EXPORT NSString* const JsTensorTypeString;
 
++ (Ort::Value)createInputTensorJSI:(facebook::jsi::Runtime &)runtime
+                   input:(const facebook::jsi::Object *)input
+                   ortAllocator:(OrtAllocator *)ortAllocator
+                    allocations:(std::vector<Ort::MemoryAllocation> &)allocatons;
+
 /**
  * It creates an input tensor from a map passed by react native js.
  * 'data' must be a string type as data is encoded as base64. It first decodes it and creates a tensor.
@@ -30,6 +36,10 @@ FOUNDATION_EXPORT NSString* const JsTensorTypeString;
                   ortAllocator:(OrtAllocator*)ortAllocator
                    allocations:(std::vector<Ort::MemoryAllocation>&)allocatons;
 
++(facebook::jsi::Object)createOutputTensorJSI:(facebook::jsi::Runtime &)runtime
+                            outputNames:(const std::vector<const char*>&)outputNames
+                            values:(const std::vector<Ort::Value>&)values;
+
 /**
  * It creates an output map from an output tensor.
  * a data array is encoded as base64 string.
@@ -37,6 +47,7 @@ FOUNDATION_EXPORT NSString* const JsTensorTypeString;
 +(NSDictionary*)createOutputTensor:(const std::vector<const char*>&)outputNames
                             values:(const std::vector<Ort::Value>&)values;
 
+
 @end
 
 #endif /* TensorHelper_h */
diff --git a/node_modules/onnxruntime-react-native/ios/TensorHelper.mm b/node_modules/onnxruntime-react-native/ios/TensorHelper.mm
index 00c1c79..4d48ac3 100644
--- a/node_modules/onnxruntime-react-native/ios/TensorHelper.mm
+++ b/node_modules/onnxruntime-react-native/ios/TensorHelper.mm
@@ -3,6 +3,8 @@
 
 #import "TensorHelper.h"
 #import <Foundation/Foundation.h>
+#import <React/RCTBridge+Private.h>
+#import <React/RCTBlobManager.h>
 
 @implementation TensorHelper
 
@@ -48,8 +50,12 @@ @implementation TensorHelper
     }
     return inputTensor;
   } else {
-    NSString *data = [input objectForKey:@"data"];
-    NSData *buffer = [[NSData alloc] initWithBase64EncodedString:data options:0];
+    NSDictionary *data = [input objectForKey:@"data"];
+    NSString *blobId = [data objectForKey:@"blobId"];
+    long size = [[data objectForKey:@"size"] longValue];
+    long offset = [[data objectForKey:@"offset"] longValue];
+    RCTBlobManager* blobManager = [[RCTBridge currentBridge] moduleForClass:RCTBlobManager.class];
+    auto buffer = [blobManager resolve:blobId offset:offset size:size];
     Ort::Value inputTensor = [self createInputTensor:tensorType
                                                 dims:dims
                                               buffer:buffer
@@ -109,8 +115,14 @@ + (NSDictionary *)createOutputTensor:(const std::vector<const char *> &)outputNa
       }
       outputTensor[@"data"] = buffer;
     } else {
-      NSString *data = [self createOutputTensor:value];
-      outputTensor[@"data"] = data;
+      NSData *data = [self createOutputTensor:value];
+      RCTBlobManager* blobManager = [[RCTBridge currentBridge] moduleForClass:RCTBlobManager.class];
+      NSString* blobId = [blobManager store:data];
+      outputTensor[@"data"] = @{
+        @"blobId": blobId,
+        @"offset": @0,
+        @"size": @(data.length),
+      };
     }
 
     outputTensorMap[[NSString stringWithUTF8String:outputName]] = outputTensor;
@@ -170,15 +182,14 @@ + (NSDictionary *)createOutputTensor:(const std::vector<const char *> &)outputNa
   }
 }
 
-template <typename T> static NSString *createOutputTensorT(const Ort::Value &tensor) {
+template <typename T> static NSData *createOutputTensorT(const Ort::Value &tensor) {
   const auto data = tensor.GetTensorData<T>();
-  NSData *buffer = [NSData dataWithBytesNoCopy:(void *)data
+  return [NSData dataWithBytesNoCopy:(void *)data
                                         length:tensor.GetTensorTypeAndShapeInfo().GetElementCount() * sizeof(T)
                                   freeWhenDone:false];
-  return [buffer base64EncodedStringWithOptions:0];
 }
 
-+ (NSString *)createOutputTensor:(const Ort::Value &)tensor {
++ (NSData *)createOutputTensor:(const Ort::Value &)tensor {
   ONNXTensorElementDataType tensorType = tensor.GetTensorTypeAndShapeInfo().GetElementType();
 
   switch (tensorType) {
diff --git a/node_modules/onnxruntime-react-native/lib/backend.ts b/node_modules/onnxruntime-react-native/lib/backend.ts
index 4ebc364..a2812c0 100644
--- a/node_modules/onnxruntime-react-native/lib/backend.ts
+++ b/node_modules/onnxruntime-react-native/lib/backend.ts
@@ -48,25 +48,35 @@ class OnnxruntimeSessionHandler implements SessionHandler {
   #inferenceSession: Binding.InferenceSession;
   #key: string;
 
+  #pathOrBuffer: string|Uint8Array;
+
   inputNames: string[];
   outputNames: string[];
 
-  constructor(path: string) {
+  constructor(pathOrBuffer: string|Uint8Array) {
     this.#inferenceSession = binding;
-    this.#key = normalizePath(path);
+    this.#pathOrBuffer = pathOrBuffer;
+    this.#key = '';
+
     this.inputNames = [];
     this.outputNames = [];
   }
 
   async loadModel(options: InferenceSession.SessionOptions): Promise<void> {
     try {
+      let results: Binding.ModelLoadInfoType;
       // load a model
-      const results: Binding.ModelLoadInfoType = await this.#inferenceSession.loadModel(this.#key, options);
-      // resolve promise if onnxruntime session is successfully created
-      if (results.key !== this.#key) {
-        throw new Error('Session key is invalid');
+      if (typeof this.#pathOrBuffer === 'string') {
+        results = await this.#inferenceSession.loadModel(normalizePath(this.#pathOrBuffer), options);
+      } else {
+        if (!this.#inferenceSession.loadModelFromBlob) {
+          throw new Error('Native module method "loadModelFromBlob" is not defined');
+        }
+        const modelBlob = global.__ONNXRUNTIME_setTensorArrayBuffer(this.#pathOrBuffer);
+        results = await this.#inferenceSession.loadModelFromBlob(modelBlob, options);
       }
-
+      // resolve promise if onnxruntime session is successfully created
+      this.#key = results.key;
       this.inputNames = results.inputNames;
       this.outputNames = results.outputNames;
     } catch (e) {
@@ -97,9 +107,12 @@ class OnnxruntimeSessionHandler implements SessionHandler {
         outputNames.push(name);
       }
     }
+    const t0 = performance.now();
     const input = this.encodeFeedsType(feeds);
     const results: Binding.ReturnType = await this.#inferenceSession.run(this.#key, input, outputNames, options);
     const output = this.decodeReturnType(results);
+    console.log(`run took ${performance.now() - t0} ms`)
+    
     return output;
   }
 
@@ -112,9 +125,8 @@ class OnnxruntimeSessionHandler implements SessionHandler {
         if (Array.isArray(feeds[key].data)) {
           data = feeds[key].data as string[];
         } else {
-          // Base64-encode tensor data
           const buffer = (feeds[key].data as SupportedTypedArray).buffer;
-          data = Buffer.from(buffer, 0, buffer.byteLength).toString('base64');
+          data = global.__ONNXRUNTIME_setTensorArrayBuffer(buffer)
         }
 
         returnValue[key] = {
@@ -136,7 +148,7 @@ class OnnxruntimeSessionHandler implements SessionHandler {
         if (Array.isArray(results[key].data)) {
           tensorData = results[key].data as string[];
         } else {
-          const buffer: Buffer = Buffer.from(results[key].data as string, 'base64');
+          const buffer: Buffer = Buffer.from(global.__ONNXRUNTIME_getTensorArrayBuffer(results[key].data));
           const typedArray = tensorTypeToTypedArray(results[key].type as Tensor.Type);
           tensorData = new typedArray(buffer.buffer, buffer.byteOffset, buffer.length / typedArray.BYTES_PER_ELEMENT);
         }
@@ -156,9 +168,6 @@ class OnnxruntimeBackend implements Backend {
 
   async createSessionHandler(pathOrBuffer: string|Uint8Array, options?: InferenceSession.SessionOptions):
       Promise<SessionHandler> {
-    if (typeof pathOrBuffer !== 'string') {
-      throw new Error('Uint8Array is not supported');
-    }
     const handler = new OnnxruntimeSessionHandler(pathOrBuffer);
     await handler.loadModel(options || {});
     return handler;
diff --git a/node_modules/onnxruntime-react-native/lib/binding.ts b/node_modules/onnxruntime-react-native/lib/binding.ts
index afadbab..cb5d023 100644
--- a/node_modules/onnxruntime-react-native/lib/binding.ts
+++ b/node_modules/onnxruntime-react-native/lib/binding.ts
@@ -64,6 +64,7 @@ export declare namespace Binding {
 
   interface InferenceSession {
     loadModel(modelPath: string, options: SessionOptions): Promise<ModelLoadInfoType>;
+    loadModelFromBase64EncodedBuffer?(buffer: string, options: SessionOptions): Promise<ModelLoadInfoType>;
     run(key: string, feeds: FeedsType, fetches: FetchesType, options: RunOptions): Promise<ReturnType>;
   }
 }
@@ -71,3 +72,5 @@ export declare namespace Binding {
 // export native binding
 const {Onnxruntime} = NativeModules;
 export const binding = Onnxruntime as Binding.InferenceSession;
+
+Onnxruntime.install();
diff --git a/node_modules/onnxruntime-react-native/onnxruntime-react-native.podspec b/node_modules/onnxruntime-react-native/onnxruntime-react-native.podspec
index aeb08b3..1f499fc 100644
--- a/node_modules/onnxruntime-react-native/onnxruntime-react-native.podspec
+++ b/node_modules/onnxruntime-react-native/onnxruntime-react-native.podspec
@@ -15,8 +15,10 @@ Pod::Spec.new do |spec|
   spec.platforms            = { :ios => "12.4" }
   spec.source               = { :git => "https://github.com/Microsoft/onnxruntime.git", :tag => "rel-#{spec.version}" }
 
-  spec.source_files         = "ios/*.{h,mm}"
+  spec.source_files         = "ios/*.{h,mm}", "cpp/*.{h,cpp}"
 
   spec.dependency "React-Core"
+  spec.dependency "React-callinvoker"
+  spec.dependency "ReactCommon/turbomodule/core"
   spec.dependency "onnxruntime-c"
 end
